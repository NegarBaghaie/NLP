{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c67c32-af31-46e6-856f-45eee6e28f0d",
   "metadata": {},
   "source": [
    "## Shahnameh Word-Level Text Generation\n",
    "\n",
    "This project focuses on creating a text generation model inspired by the Shahnameh, an epic poem by the Persian poet Ferdowsi. By utilizing natural language processing (NLP) techniques, the model generates text at the word level, aiming to replicate the intricate style and rich content of the original work. The model is trained on the Shahnameh's extensive corpus to understand its language patterns and themes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7a6418-c77b-4de8-a6e9-2bf9097a307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '20'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import keras\n",
    "from keras.layers import GRU, Dropout, Dense, Embedding, TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53cb578-dcc6-4aa2-92ec-5df458822036",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ef0c50-0ca9-469d-8649-696799c2867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'archive/shahname_fa.txt'\n",
    "def read_text(path):\n",
    "  with open(path, 'rb') as file:\n",
    "    return file.read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18812d77-3b1b-4f52-b34e-5ab17c5a0f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|خداوند کیوان و گردان سپهر\\n|فروزنده ماه و ناهید و مهر\\n|ز نام و نشان و گمان برترست\\n|نگارندهٔ بر شده پیکرست\\n|به بینندگان آفریننده را\\n|نبینی مرنجان دو بیننده را\\n|نیابد بدو نیز اندیشه راه\\n|که او برتر از نا'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = read_text(path)\n",
    "text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5cb515-bd9f-4a7e-821e-816faa1dd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2653849"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb0ed8-a439-40bf-8af9-80e8281d30c8",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- **Custom Standardization:** Removes zero-width non-joiners, adds spaces around pipes (|), and replaces newlines with <newline>.\n",
    "\n",
    "- **Max Tokens:** Limits the number of tokens to MAX_TOKEN, set at 19,500.\n",
    "\n",
    "- **TextVectorization Layer:** Configured to use the custom standardization, output integer tokens, and limit the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b60a3a6-2c21-4fb3-992d-96b59a2dcb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(text):\n",
    "    text = tf.strings.regex_replace(text, '\\u200c', '')\n",
    "    text = tf.strings.regex_replace(text, r'\\|', ' | ')\n",
    "    text = tf.strings.regex_replace(text, '\\n', ' <newline> ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697bc021-8d67-4716-a1e2-d20ff6ddc429",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN = 19500\n",
    "text_vec = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=MAX_TOKEN\n",
    ")\n",
    "\n",
    "text_vec.adapt(tf.constant([text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b553ef-1403-48bd-b1ad-c7fbba6e0dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '|',\n",
       " '<newline>',\n",
       " 'و',\n",
       " 'به',\n",
       " 'که',\n",
       " 'ز',\n",
       " 'از',\n",
       " 'بر',\n",
       " 'را',\n",
       " 'چو',\n",
       " 'با',\n",
       " 'گفت',\n",
       " 'شد',\n",
       " 'تو',\n",
       " 'شاه',\n",
       " 'همی',\n",
       " 'بود',\n",
       " 'او',\n",
       " 'یکی',\n",
       " 'همه',\n",
       " 'آن',\n",
       " 'من',\n",
       " 'اندر',\n",
       " 'در',\n",
       " 'تا',\n",
       " 'سر',\n",
       " 'چنین',\n",
       " 'کرد',\n",
       " 'آمد',\n",
       " 'دل',\n",
       " 'پیش',\n",
       " 'این',\n",
       " 'جهان',\n",
       " 'بد',\n",
       " 'پر',\n",
       " 'بدو',\n",
       " 'هر',\n",
       " 'سپاه',\n",
       " 'چون',\n",
       " 'داد',\n",
       " 'پس',\n",
       " 'دو',\n",
       " 'نه',\n",
       " 'سوی',\n",
       " 'راه',\n",
       " 'کار',\n",
       " 'لشکر',\n",
       " 'روی',\n",
       " 'چه',\n",
       " 'تخت',\n",
       " 'سخن',\n",
       " 'ما',\n",
       " 'مرد',\n",
       " 'مرا',\n",
       " 'گر',\n",
       " 'جای',\n",
       " 'هم',\n",
       " 'زمین',\n",
       " 'گرد',\n",
       " 'جنگ',\n",
       " 'دست',\n",
       " 'ایران',\n",
       " 'گشت',\n",
       " 'بیامد',\n",
       " 'همان',\n",
       " 'روز',\n",
       " 'باد',\n",
       " 'یک',\n",
       " 'کسی',\n",
       " 'شهریار',\n",
       " 'بدین',\n",
       " 'بدان',\n",
       " 'خویش',\n",
       " 'ازان',\n",
       " 'کس',\n",
       " 'تاج',\n",
       " 'اگر',\n",
       " 'رای',\n",
       " 'ای',\n",
       " 'تن',\n",
       " 'اوی',\n",
       " 'گنج',\n",
       " 'بران',\n",
       " 'خون',\n",
       " 'نام',\n",
       " 'کنون',\n",
       " 'نزدیک',\n",
       " 'زین',\n",
       " 'آفرین',\n",
       " 'نیست',\n",
       " 'رنج',\n",
       " 'آب',\n",
       " 'برو',\n",
       " 'کوه',\n",
       " 'نیز',\n",
       " 'کین',\n",
       " 'چنان',\n",
       " 'شود',\n",
       " 'پاسخ',\n",
       " 'رستم',\n",
       " 'اندرون',\n",
       " 'زان',\n",
       " 'خرد',\n",
       " 'دید',\n",
       " 'گیتی',\n",
       " 'شیر',\n",
       " 'یزدان',\n",
       " 'آید',\n",
       " 'میان',\n",
       " 'دگر',\n",
       " 'شب',\n",
       " 'ترا',\n",
       " 'جز',\n",
       " 'بهرام',\n",
       " 'برین',\n",
       " 'جان',\n",
       " 'شاد',\n",
       " 'درد',\n",
       " 'کجا',\n",
       " 'خاک',\n",
       " 'بسی',\n",
       " 'دشت',\n",
       " 'سپه',\n",
       " 'زمان',\n",
       " 'یاد',\n",
       " 'بهر',\n",
       " 'گفتار',\n",
       " 'باید',\n",
       " 'باشد',\n",
       " 'کای',\n",
       " 'ماه',\n",
       " 'برآمد',\n",
       " 'بفرمود',\n",
       " 'پدر',\n",
       " 'آورد',\n",
       " 'پای',\n",
       " 'باز',\n",
       " 'روان',\n",
       " 'مگر',\n",
       " 'نامه',\n",
       " 'تیره',\n",
       " 'کند',\n",
       " 'خسرو',\n",
       " 'روزگار',\n",
       " 'ازین',\n",
       " 'وز',\n",
       " 'بشنید',\n",
       " 'شهر',\n",
       " 'جهاندار',\n",
       " 'خود',\n",
       " 'انجمن',\n",
       " 'مهر',\n",
       " 'گردان',\n",
       " 'خورشید',\n",
       " 'پهلوان',\n",
       " 'کز',\n",
       " 'پشت',\n",
       " 'تیغ',\n",
       " 'گفتی',\n",
       " 'نامور',\n",
       " 'گاه',\n",
       " 'فرمان',\n",
       " 'وزان',\n",
       " 'ورا',\n",
       " 'پیل',\n",
       " 'رزم',\n",
       " 'بخت',\n",
       " 'پاک',\n",
       " 'بدی',\n",
       " 'رفت',\n",
       " 'بلند',\n",
       " 'ازو',\n",
       " 'کن',\n",
       " 'چین',\n",
       " 'سوار',\n",
       " 'نبرد',\n",
       " 'بزرگان',\n",
       " 'نگه',\n",
       " 'دارد',\n",
       " 'کلاه',\n",
       " 'برفتند',\n",
       " 'رسید',\n",
       " 'گردد',\n",
       " 'راست',\n",
       " 'نهان',\n",
       " 'نزد',\n",
       " 'گونه',\n",
       " 'بند',\n",
       " 'گیو',\n",
       " 'بوم',\n",
       " 'زو',\n",
       " 'وگر',\n",
       " 'نشست',\n",
       " 'می',\n",
       " 'آتش',\n",
       " 'نامداران',\n",
       " 'زرین',\n",
       " 'کردار',\n",
       " 'خواب',\n",
       " 'چشم',\n",
       " 'نباید',\n",
       " 'سواران',\n",
       " 'سه',\n",
       " 'شدند',\n",
       " 'افراسیاب',\n",
       " 'کنم',\n",
       " 'سپهبد',\n",
       " 'مر',\n",
       " 'گرفت',\n",
       " 'دیگر',\n",
       " 'کو',\n",
       " 'تیر',\n",
       " 'دور',\n",
       " 'تنگ',\n",
       " 'زر',\n",
       " 'اسپ',\n",
       " 'خردمند',\n",
       " 'زن',\n",
       " 'مردم',\n",
       " 'سرش',\n",
       " 'بدید',\n",
       " 'زیر',\n",
       " 'پیران',\n",
       " 'نامدار',\n",
       " 'فرستاد',\n",
       " 'چنگ',\n",
       " 'تیز',\n",
       " 'کارزار',\n",
       " 'ایوان',\n",
       " 'سیاوش',\n",
       " 'فر',\n",
       " 'چند',\n",
       " 'نیک',\n",
       " 'روشن',\n",
       " 'کام',\n",
       " 'آواز',\n",
       " 'پی',\n",
       " 'فرزند',\n",
       " 'کشته',\n",
       " 'توران',\n",
       " 'فراوان',\n",
       " 'جوان',\n",
       " 'بیدار',\n",
       " 'کمر',\n",
       " 'دانش',\n",
       " 'بگفت',\n",
       " 'ابا',\n",
       " 'نو',\n",
       " 'یکسر',\n",
       " 'نماند',\n",
       " 'کشید',\n",
       " 'گوهر',\n",
       " 'سپهر',\n",
       " 'دیده',\n",
       " 'پیر',\n",
       " 'گران',\n",
       " 'هزار',\n",
       " 'آنک',\n",
       " 'قیصر',\n",
       " 'ده',\n",
       " 'بار',\n",
       " 'دلش',\n",
       " 'گودرز',\n",
       " 'کرده',\n",
       " 'پسر',\n",
       " 'راز',\n",
       " 'ترکان',\n",
       " 'برد',\n",
       " 'مرز',\n",
       " 'سو',\n",
       " 'بس',\n",
       " 'جنگی',\n",
       " 'فرستاده',\n",
       " 'چیز',\n",
       " 'بباید',\n",
       " 'ماند',\n",
       " 'طوس',\n",
       " 'ایرانیان',\n",
       " 'است',\n",
       " 'روم',\n",
       " 'ایدر',\n",
       " 'درفش',\n",
       " 'چرخ',\n",
       " 'هرچ',\n",
       " 'شاهی',\n",
       " 'خیره',\n",
       " 'شما',\n",
       " 'بشد',\n",
       " 'نهادند',\n",
       " 'دلیر',\n",
       " 'فرود',\n",
       " 'دشمن',\n",
       " 'سالار',\n",
       " 'آیین',\n",
       " 'آمدند',\n",
       " 'بیاورد',\n",
       " 'ایشان',\n",
       " 'هوا',\n",
       " 'فرخ',\n",
       " 'اندیشه',\n",
       " 'گرز',\n",
       " 'کینه',\n",
       " 'پیروز',\n",
       " 'مردی',\n",
       " 'سان',\n",
       " 'هرگز',\n",
       " 'سپهدار',\n",
       " 'نباشد',\n",
       " 'خوار',\n",
       " 'فراز',\n",
       " 'لب',\n",
       " 'سپاهی',\n",
       " 'خواند',\n",
       " 'جفت',\n",
       " 'چرا',\n",
       " 'موبد',\n",
       " 'خواست',\n",
       " 'ابر',\n",
       " 'کوس',\n",
       " 'غم',\n",
       " 'زبان',\n",
       " 'رخ',\n",
       " 'خاقان',\n",
       " 'بپرسید',\n",
       " 'اکنون',\n",
       " 'آرام',\n",
       " 'شمشیر',\n",
       " 'بخواند',\n",
       " 'سیاه',\n",
       " 'بیژن',\n",
       " 'پدید',\n",
       " 'بماند',\n",
       " 'نهاد',\n",
       " 'دینار',\n",
       " 'خروش',\n",
       " 'نهفت',\n",
       " 'کشور',\n",
       " 'بسته',\n",
       " 'دراز',\n",
       " 'چندی',\n",
       " 'دین',\n",
       " 'درم',\n",
       " 'دریا',\n",
       " 'سراسر',\n",
       " 'جام',\n",
       " 'اسفندیار',\n",
       " 'گوی',\n",
       " 'مکن',\n",
       " 'دار',\n",
       " 'پیمان',\n",
       " 'رزمگاه',\n",
       " 'بزد',\n",
       " 'کمان',\n",
       " 'نیزه',\n",
       " 'خداوند',\n",
       " 'باره',\n",
       " 'کهن',\n",
       " 'کزین',\n",
       " 'رنگ',\n",
       " 'دیو',\n",
       " 'نژاد',\n",
       " 'سرافراز',\n",
       " 'جوی',\n",
       " 'هیچ',\n",
       " 'سام',\n",
       " 'سرای',\n",
       " 'مهتر',\n",
       " 'شنید',\n",
       " 'بیش',\n",
       " 'نخست',\n",
       " 'سال',\n",
       " 'هست',\n",
       " 'زال',\n",
       " 'نشان',\n",
       " 'باش',\n",
       " 'مهان',\n",
       " 'زرد',\n",
       " 'رود',\n",
       " 'خواسته',\n",
       " 'براند',\n",
       " 'مرگ',\n",
       " 'صد',\n",
       " 'بنهاد',\n",
       " 'آنچ',\n",
       " 'کی',\n",
       " 'دمان',\n",
       " 'داستان',\n",
       " 'بسیار',\n",
       " 'زد',\n",
       " 'داشت',\n",
       " 'بدست',\n",
       " 'پور',\n",
       " 'سرو',\n",
       " 'سران',\n",
       " 'بزرگی',\n",
       " 'گزین',\n",
       " 'ندید',\n",
       " 'مشک',\n",
       " 'شده',\n",
       " 'دم',\n",
       " 'دژ',\n",
       " 'کم',\n",
       " 'یار',\n",
       " 'اختر',\n",
       " 'پیاده',\n",
       " 'زمانه',\n",
       " 'جست',\n",
       " 'گوش',\n",
       " 'گشته',\n",
       " 'شوم',\n",
       " 'نیاید',\n",
       " 'آباد',\n",
       " 'کاووس',\n",
       " 'چهر',\n",
       " 'دلیران',\n",
       " 'گزند',\n",
       " 'خسته',\n",
       " 'کاین',\n",
       " 'نیکی',\n",
       " 'موبدان',\n",
       " 'بیاراست',\n",
       " 'سخنها',\n",
       " 'درگاه',\n",
       " 'خورد',\n",
       " 'خشم',\n",
       " 'نبود',\n",
       " 'رهنمای',\n",
       " 'مهتران',\n",
       " 'دانا',\n",
       " 'نبد',\n",
       " 'برش',\n",
       " 'دیدار',\n",
       " 'سنگ',\n",
       " 'بیم',\n",
       " 'بالا',\n",
       " 'کردگار',\n",
       " 'چاره',\n",
       " 'پند',\n",
       " 'منست',\n",
       " 'شگفت',\n",
       " 'رخش',\n",
       " 'کنیم',\n",
       " 'کنی',\n",
       " 'شاهان',\n",
       " 'گشتاسپ',\n",
       " 'سکندر',\n",
       " 'دژم',\n",
       " 'برفت',\n",
       " 'خواهد',\n",
       " 'بیرون',\n",
       " 'برادر',\n",
       " 'رومی',\n",
       " 'تهمتن',\n",
       " 'بگشاد',\n",
       " 'آگهی',\n",
       " 'سواری',\n",
       " 'درنگ',\n",
       " 'کنید',\n",
       " 'جایگه',\n",
       " 'بزرگ',\n",
       " 'همانا',\n",
       " 'راستی',\n",
       " 'بن',\n",
       " 'آمدش',\n",
       " 'گه',\n",
       " 'کاخ',\n",
       " 'چندین',\n",
       " 'پراگنده',\n",
       " 'ساز',\n",
       " 'برآورد',\n",
       " 'کمند',\n",
       " 'ندارد',\n",
       " 'خویشتن',\n",
       " 'گویی',\n",
       " 'شادی',\n",
       " 'گردش',\n",
       " 'چیزی',\n",
       " 'مغز',\n",
       " 'زنده',\n",
       " 'ره',\n",
       " 'تنش',\n",
       " 'بجنگ',\n",
       " 'پلنگ',\n",
       " 'هنگام',\n",
       " 'همیشه',\n",
       " 'گل',\n",
       " 'ننگ',\n",
       " 'آسمان',\n",
       " 'کان',\n",
       " 'سپرد',\n",
       " 'خواهی',\n",
       " 'خواندند',\n",
       " 'گذشت',\n",
       " 'اسب',\n",
       " 'گو',\n",
       " 'اردشیر',\n",
       " 'پادشاهی',\n",
       " 'سخت',\n",
       " 'داری',\n",
       " 'خوب',\n",
       " 'گور',\n",
       " 'ناپدید',\n",
       " 'کیان',\n",
       " 'شهنشاه',\n",
       " 'بی',\n",
       " 'هنر',\n",
       " 'مردان',\n",
       " 'درخت',\n",
       " 'جهانجوی',\n",
       " 'تست',\n",
       " 'تیمار',\n",
       " 'گروه',\n",
       " 'پیوند',\n",
       " 'پرستنده',\n",
       " 'برکشید',\n",
       " 'اژدها',\n",
       " 'گرفتند',\n",
       " 'گرانمایه',\n",
       " 'نگاه',\n",
       " 'بکردار',\n",
       " 'آرد',\n",
       " 'یکایک',\n",
       " 'زور',\n",
       " 'دادگر',\n",
       " 'آنکس',\n",
       " 'کیست',\n",
       " 'ایچ',\n",
       " 'پادشا',\n",
       " 'نیامد',\n",
       " 'دریای',\n",
       " 'هوش',\n",
       " 'لشکری',\n",
       " 'دهد',\n",
       " 'برون',\n",
       " 'سخنهای',\n",
       " 'درست',\n",
       " 'خانه',\n",
       " 'بودند',\n",
       " 'افسر',\n",
       " 'گذر',\n",
       " 'بوی',\n",
       " 'کزو',\n",
       " 'زود',\n",
       " 'بگذرد',\n",
       " 'بنشست',\n",
       " 'بالای',\n",
       " 'سزد',\n",
       " 'پیکار',\n",
       " 'مایه',\n",
       " 'دستان',\n",
       " 'جایی',\n",
       " 'تخم',\n",
       " 'برگشت',\n",
       " 'یال',\n",
       " 'باشی',\n",
       " 'هفته',\n",
       " 'فریدون',\n",
       " 'زار',\n",
       " 'افگند',\n",
       " 'عنان',\n",
       " 'بداد',\n",
       " 'باغ',\n",
       " 'اندران',\n",
       " 'شرم',\n",
       " 'رو',\n",
       " 'بگوی',\n",
       " 'بودی',\n",
       " 'بجای',\n",
       " 'کزان',\n",
       " 'کردن',\n",
       " 'بنزدیک',\n",
       " 'گستهم',\n",
       " 'چیست',\n",
       " 'ببست',\n",
       " 'ار',\n",
       " 'کردند',\n",
       " 'جوشن',\n",
       " 'بنده',\n",
       " 'هامون',\n",
       " 'عاج',\n",
       " 'بسان',\n",
       " 'گوید',\n",
       " 'گردن',\n",
       " 'رفتن',\n",
       " 'گهر',\n",
       " 'شوی',\n",
       " 'باژ',\n",
       " 'پذیره',\n",
       " 'دستور',\n",
       " 'بروی',\n",
       " 'پیدا',\n",
       " 'منم',\n",
       " 'مبادا',\n",
       " 'مادر',\n",
       " 'غمی',\n",
       " 'شتاب',\n",
       " 'زمانی',\n",
       " 'چندان',\n",
       " 'پیلان',\n",
       " 'زهر',\n",
       " 'آفتاب',\n",
       " 'نشاید',\n",
       " 'مست',\n",
       " 'شدن',\n",
       " 'دبیر',\n",
       " 'اندرین',\n",
       " 'ازیشان',\n",
       " 'گرم',\n",
       " 'شاپور',\n",
       " 'خرم',\n",
       " 'ترک',\n",
       " 'پیلتن',\n",
       " 'فرو',\n",
       " 'مهتری',\n",
       " 'زخم',\n",
       " 'بزم',\n",
       " 'آرزو',\n",
       " 'کش',\n",
       " 'کردی',\n",
       " 'بیاراستند',\n",
       " 'آراسته',\n",
       " 'دلاور',\n",
       " 'خوبی',\n",
       " 'بیاید',\n",
       " 'برخاست',\n",
       " 'بانگ',\n",
       " 'نگر',\n",
       " 'نبیند',\n",
       " 'سزاوار',\n",
       " 'بیداد',\n",
       " 'کنند',\n",
       " 'فزون',\n",
       " 'هرانکس',\n",
       " 'نهاده',\n",
       " 'خواه',\n",
       " 'گرگ',\n",
       " 'کودک',\n",
       " 'هومان',\n",
       " 'هندی',\n",
       " 'خوان',\n",
       " 'بدانست',\n",
       " 'شیران',\n",
       " 'راند',\n",
       " 'اسپان',\n",
       " 'شمار',\n",
       " 'جوید',\n",
       " 'بپیچید',\n",
       " 'بارگاه',\n",
       " 'دوان',\n",
       " 'تاریک',\n",
       " 'بدند',\n",
       " 'ببر',\n",
       " 'ژیان',\n",
       " 'هرکس',\n",
       " 'موی',\n",
       " 'خنجر',\n",
       " 'بپوشید',\n",
       " 'برسان',\n",
       " 'آوریم',\n",
       " 'یلان',\n",
       " 'مه',\n",
       " 'بودش',\n",
       " 'آورید',\n",
       " 'یافت',\n",
       " 'روزی',\n",
       " 'تازه',\n",
       " 'کسری',\n",
       " 'شگفتی',\n",
       " 'خروشان',\n",
       " 'جهاندیده',\n",
       " 'بیابان',\n",
       " 'بدیشان',\n",
       " 'ببرد',\n",
       " 'آنگه',\n",
       " 'وی',\n",
       " 'ستاره',\n",
       " 'زره',\n",
       " 'خروشی',\n",
       " 'بیند',\n",
       " 'بخندید',\n",
       " 'آگاهی',\n",
       " 'نیو',\n",
       " 'تهی',\n",
       " 'برتر',\n",
       " 'پرده',\n",
       " 'وزین',\n",
       " 'نیاز',\n",
       " 'نخچیر',\n",
       " 'درود',\n",
       " 'گناه',\n",
       " 'کیخسرو',\n",
       " 'شاخ',\n",
       " 'سیه',\n",
       " 'جگر',\n",
       " 'بخردان',\n",
       " 'سرد',\n",
       " 'دختر',\n",
       " 'دان',\n",
       " 'ترگ',\n",
       " 'برآشفت',\n",
       " 'شدی',\n",
       " 'سترگ',\n",
       " 'دارم',\n",
       " 'جامه',\n",
       " 'تندی',\n",
       " 'ببستند',\n",
       " 'امروز',\n",
       " 'نامهٔ',\n",
       " 'آهن',\n",
       " 'یابد',\n",
       " 'سی',\n",
       " 'بنه',\n",
       " 'ایدونک',\n",
       " 'آمدی',\n",
       " 'کشوری',\n",
       " 'فرجام',\n",
       " 'شست',\n",
       " 'شاهنشهی',\n",
       " 'سیم',\n",
       " 'بیشه',\n",
       " 'اوست',\n",
       " 'نگردد',\n",
       " 'نخستین',\n",
       " 'فرمود',\n",
       " 'شارستان',\n",
       " 'خور',\n",
       " 'خواستند',\n",
       " 'خدای',\n",
       " 'چینی',\n",
       " 'سلیح',\n",
       " 'سبک',\n",
       " 'بپیش',\n",
       " 'برگرفت',\n",
       " 'بهم',\n",
       " 'کژی',\n",
       " 'نشسته',\n",
       " 'قباد',\n",
       " 'خورش',\n",
       " 'ببردند',\n",
       " 'آیدت',\n",
       " 'یاقوت',\n",
       " 'میدان',\n",
       " 'لشکرش',\n",
       " 'سیر',\n",
       " 'یادگار',\n",
       " 'گهی',\n",
       " 'کشتی',\n",
       " 'نیم',\n",
       " 'دلت',\n",
       " 'تفت',\n",
       " 'بهره',\n",
       " 'بهار',\n",
       " 'ارجمند',\n",
       " 'گریان',\n",
       " 'نگهبان',\n",
       " 'طلایه',\n",
       " 'سنان',\n",
       " 'آزاد',\n",
       " 'گاو',\n",
       " 'پیشش',\n",
       " 'هرک',\n",
       " 'فرخنده',\n",
       " 'سربسر',\n",
       " 'جایگاه',\n",
       " 'گردنکشان',\n",
       " 'نرم',\n",
       " 'سرانجام',\n",
       " 'دود',\n",
       " 'دلم',\n",
       " 'پیام',\n",
       " 'هند',\n",
       " 'هفت',\n",
       " 'شادمان',\n",
       " 'سپید',\n",
       " 'درویش',\n",
       " 'دانی',\n",
       " 'خم',\n",
       " 'بگذشت',\n",
       " 'بادا',\n",
       " 'ایمن',\n",
       " 'گیرد',\n",
       " 'نشستند',\n",
       " 'بنمود',\n",
       " 'بدگمان',\n",
       " 'آورم',\n",
       " 'آوردگاه',\n",
       " 'کاوس',\n",
       " 'توی',\n",
       " 'مرغ',\n",
       " 'شدست',\n",
       " 'رسم',\n",
       " 'تباه',\n",
       " 'یل',\n",
       " 'گشتند',\n",
       " 'چاک',\n",
       " 'هرآنکس',\n",
       " 'جهانی',\n",
       " 'بگویم',\n",
       " 'بارهٔ',\n",
       " 'آگاه',\n",
       " 'گفتن',\n",
       " 'نیایش',\n",
       " 'نماز',\n",
       " 'شادان',\n",
       " 'دام',\n",
       " 'بهشت',\n",
       " 'گرفته',\n",
       " 'ویران',\n",
       " 'نیابد',\n",
       " 'روشنروان',\n",
       " 'جستن',\n",
       " 'بکشت',\n",
       " 'بریشان',\n",
       " 'بخشش',\n",
       " 'آشکار',\n",
       " 'گرسیوز',\n",
       " 'مباد',\n",
       " 'رهنمون',\n",
       " 'رسد',\n",
       " 'خاست',\n",
       " 'نشیب',\n",
       " 'نبودی',\n",
       " 'لاژورد',\n",
       " 'دستگاه',\n",
       " 'برز',\n",
       " 'اندازه',\n",
       " 'آوای',\n",
       " 'گذشته',\n",
       " 'همانگه',\n",
       " 'سپاس',\n",
       " 'دیبای',\n",
       " 'خلعت',\n",
       " 'بیگمان',\n",
       " 'برنشست',\n",
       " 'براه',\n",
       " 'همچو',\n",
       " 'مهی',\n",
       " 'دیبا',\n",
       " 'دلی',\n",
       " 'بپای',\n",
       " 'بداندیش',\n",
       " 'امید',\n",
       " 'پنج',\n",
       " 'رها',\n",
       " 'دیوان',\n",
       " 'خشک',\n",
       " 'برگزید',\n",
       " 'پوزش',\n",
       " 'ستایش',\n",
       " 'زیان',\n",
       " 'بگفتند',\n",
       " 'بردند',\n",
       " 'ببخشید',\n",
       " 'گم',\n",
       " 'کشیدند',\n",
       " 'کرا',\n",
       " 'پیچان',\n",
       " 'ولیکن',\n",
       " 'نهنگ',\n",
       " 'فروز',\n",
       " 'صف',\n",
       " 'جدا',\n",
       " 'بسر',\n",
       " 'بدوی',\n",
       " 'بدخواه',\n",
       " 'آفرید',\n",
       " 'کهتر',\n",
       " 'نیا',\n",
       " 'داشتی',\n",
       " 'گنجور',\n",
       " 'چار',\n",
       " 'سود',\n",
       " 'درون',\n",
       " 'جامهٔ',\n",
       " 'گردون',\n",
       " 'پست',\n",
       " 'سرفراز',\n",
       " 'جنگجوی',\n",
       " 'آز',\n",
       " 'کردم',\n",
       " 'کران',\n",
       " 'منوچهر',\n",
       " 'فریبرز',\n",
       " 'سرخ',\n",
       " 'ساختند',\n",
       " 'ریخت',\n",
       " 'دیهیم',\n",
       " 'بیک',\n",
       " 'بینی',\n",
       " 'بنفش',\n",
       " 'گمان',\n",
       " 'چهارم',\n",
       " 'پیروزگر',\n",
       " 'نخواهد',\n",
       " 'داند',\n",
       " 'بشست',\n",
       " 'برزد',\n",
       " 'گفتند',\n",
       " 'گشاده',\n",
       " 'گریست',\n",
       " 'پراندیشه',\n",
       " 'هور',\n",
       " 'نفرین',\n",
       " 'شادکام',\n",
       " 'خروشیدن',\n",
       " 'جاودان',\n",
       " 'تور',\n",
       " 'بگذاشتند',\n",
       " 'نخواهم',\n",
       " 'مازندران',\n",
       " 'ضحاک',\n",
       " 'شاهوار',\n",
       " 'جوش',\n",
       " 'بست',\n",
       " 'بجز',\n",
       " 'نکرد',\n",
       " 'نوذر',\n",
       " 'نامبردار',\n",
       " 'سپر',\n",
       " 'دیر',\n",
       " 'بهمن',\n",
       " 'برزین',\n",
       " 'بدانگه',\n",
       " 'پرمایه',\n",
       " 'پرخاشجوی',\n",
       " 'ندانم',\n",
       " 'دریغ',\n",
       " 'خان',\n",
       " 'برآید',\n",
       " 'بدرد',\n",
       " 'کشتن',\n",
       " 'پیکر',\n",
       " 'ببند',\n",
       " 'آهنگ',\n",
       " 'گرامی',\n",
       " 'کاری',\n",
       " 'هرگونه',\n",
       " 'سرت',\n",
       " 'زشت',\n",
       " 'خوش',\n",
       " 'کشت',\n",
       " 'کابل',\n",
       " 'چپ',\n",
       " 'پیروزی',\n",
       " 'پولاد',\n",
       " 'نشاند',\n",
       " 'نامداری',\n",
       " 'رویین',\n",
       " 'داور',\n",
       " 'خوی',\n",
       " 'خواهم',\n",
       " 'تازی',\n",
       " 'تاختن',\n",
       " 'ببوسید',\n",
       " 'اندکی',\n",
       " 'گرگین',\n",
       " 'کنار',\n",
       " 'کاستی',\n",
       " 'پیکان',\n",
       " 'وزو',\n",
       " 'همیگفت',\n",
       " 'شکار',\n",
       " 'شه',\n",
       " 'سهراب',\n",
       " 'سرکشان',\n",
       " 'ساخت',\n",
       " 'پسند',\n",
       " 'نگین',\n",
       " 'نمود',\n",
       " 'مجوی',\n",
       " 'قلب',\n",
       " 'شیرین',\n",
       " 'بروبر',\n",
       " 'اندرو',\n",
       " 'آرزوی',\n",
       " 'گفتم',\n",
       " 'پوست',\n",
       " 'نره',\n",
       " 'نای',\n",
       " 'سور',\n",
       " 'زدند',\n",
       " 'ریختن',\n",
       " 'دیدی',\n",
       " 'دهقان',\n",
       " 'داننده',\n",
       " 'خوردن',\n",
       " 'خندان',\n",
       " 'جزین',\n",
       " 'برنهاد',\n",
       " 'باک',\n",
       " 'گردی',\n",
       " 'چاه',\n",
       " 'پهلوی',\n",
       " 'همین',\n",
       " 'نگون',\n",
       " 'نداند',\n",
       " 'نبینی',\n",
       " 'فردا',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c978add-c314-4f76-8c02-5af68f475143",
   "metadata": {},
   "source": [
    "### Converting Token IDs to Text: \n",
    "\n",
    "- The ids_to_text function converts a sequence of token IDs back into readable text using the vocabulary from the TextVectorization layer.\n",
    "- It maps each token ID to its corresponding word in the vocabulary, replaces <newline>| tokens with actual newline characters, and then joins the words into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9712d31-0398-4826-9287-76d7c5216933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_text(ids):\n",
    "    text = [text_vec.get_vocabulary()[idx] for idx in ids]\n",
    "    text = tf.strings.regex_replace(text, '<newline>', '\\n')\n",
    "    return tf.strings.reduce_join(text, separator=' ').numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00dabf47-cccf-49ad-ae4b-ff0b09525dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بود سوی تخت در \n",
      " از بر\n"
     ]
    }
   ],
   "source": [
    "ids = [18, 45, 51, 25, 3, 8, 9]\n",
    "print(ids_to_text(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dee7d-2e0c-47c5-a435-38756c29540d",
   "metadata": {},
   "source": [
    "## Create training examples and targets\n",
    "\n",
    "- Next, divide the text into example sequences. Each input sequence will contain `seq_length` tokens from the text.\n",
    "\n",
    "- For each input sequence, the corresponding targets contain the same length of text, except shifted one token to the right.\n",
    "\n",
    "- To achieve this, break the text into chunks of `seq_length + 1`. For instance, if seq_length is 4 and our text is \"Hello\", the input sequence would be \"Hell\", and the target sequence would be \"ello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2c98eb-4da9-4312-80ac-4df3d3883d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "MAX_SEQ_LEN = 10\n",
    "\n",
    "def vectorizer(text):\n",
    "    return text_vec(text)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.constant([text]))\n",
    "dataset = dataset.cache().map(vectorizer)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "seq = dataset.batch(MAX_SEQ_LEN + 1, drop_remainder=True)\n",
    "\n",
    "dataset = seq.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08849d42-4417-40f4-8f2f-db8d47104c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: | به نام خداوند جان و خرد \n",
      " | کزین\n",
      "Target: به نام خداوند جان و خرد \n",
      " | کزین برتر\n",
      "Input: اندیشه برنگذرد \n",
      " | خداوند نام و خداوند جای \n",
      "\n",
      "Target: برنگذرد \n",
      " | خداوند نام و خداوند جای \n",
      " |\n",
      "Input: خداوند روزی ده رهنمای \n",
      " | خداوند کیوان و گردان\n",
      "Target: روزی ده رهنمای \n",
      " | خداوند کیوان و گردان سپهر\n",
      "Input: \n",
      " | فروزنده ماه و ناهید و مهر \n",
      " |\n",
      "Target: | فروزنده ماه و ناهید و مهر \n",
      " | ز\n",
      "Input: نام و نشان و گمان برترست \n",
      " | نگارندهٔ بر\n",
      "Target: و نشان و گمان برترست \n",
      " | نگارندهٔ بر شده\n"
     ]
    }
   ],
   "source": [
    "for input_ids, target_ids in dataset.take(5):\n",
    "    print(\"Input:\", ids_to_text(input_ids))\n",
    "    print(\"Target:\", ids_to_text(target_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f07655-f9c5-4872-bd25-39de65cc862e",
   "metadata": {},
   "source": [
    "## Create training batches\n",
    "\n",
    "After segmenting the text into manageable sequences, the next step is to prepare the data for model training. This involves two main steps: shuffling the data and packing it into batches.\n",
    "\n",
    "Shuffling ensures that the model encounters a diverse range of examples during training, preventing it from learning any sequential patterns. Batching groups these sequences together, making training more efficient by processing multiple examples at once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30d1fc9-6077-4928-b6de-64682a14b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
    "dataset = dataset.batch(batch_size=BATCH_SIZE, num_parallel_calls=AUTOTUNE)\n",
    "dataset = dataset.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6919ae-331e-466a-9d62-251e77413ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tf.Tensor(\n",
      "[[   21   420 19286   129  1254    29     3     2    11    12]\n",
      " [    5    32   756    22  1135     3     2  1246     4  2373]\n",
      " [ 1872   450  1158     3     2    76     8  3921   451   895]\n",
      " [   21  5887  2493     8  8536     3     2    20 16443     5]\n",
      " [ 3712   598   458     3     2    14    22   532  2392 18665]\n",
      " [  438    91     3     2     6   239     4   973  1851  1316]\n",
      " [    3     2    11   144   587  1744    10  3301     3     2]\n",
      " [    3     2   863    29   760     4  2289     4  2087     3]\n",
      " [   77   155    51     3     2 15249   422    26  1894     3]\n",
      " [   18  1925     4  5884     3     2   218     4   244     4]\n",
      " [ 2076    90   212   143    90     3     2     9    22   168]\n",
      " [  106   895  8333   267     3     2    17 12076    12  2412]\n",
      " [    3     2   371     8   432  6279    47    29     3     2]\n",
      " [    2    17    13    12    38    70    79    74     3     2]\n",
      " [   49     3     2     5   286    94    90   493     3     2]\n",
      " [   18     3     2    36     8  1129    31  5296   145     3]\n",
      " [  261     4    43   773    14  1025     3     2   518    38]\n",
      " [ 1749    10     5  1556   575     3     2    11     9  3040]\n",
      " [    2    34   235     8   161   168   511     3     2    15]\n",
      " [    2     5    86   356   117     4   104     3     2   359]\n",
      " [    4   232   504     3     2     7   689   502     4  5016]\n",
      " [    7    23  2170  4019   350     3     2  8678     8   595]\n",
      " [    2    20   156    18   985   362     3     2   291     4]\n",
      " [  171    38   188     8   370     4   403     3     2     7]\n",
      " [  897     3     2     7  7000  3118   583   371     3     2]\n",
      " [   20   394   826     3     2     5   584  1303     7   946]\n",
      " [    8  5284   739     3     2     7    38  2375     8   790]\n",
      " [  312     3     2    98   105   235  1851     5   200     3]\n",
      " [    8    47   106  5340    18     3     2  5729    37    13]\n",
      " [17641     3     2    44    22    92     4   512  7900     3]\n",
      " [    3     2     5   155  1045  3374     3     2  3619  4398]\n",
      " [    8   337     3     2     6    23   533    29   952    17]\n",
      " [    9   558    43  1361     3     2     7  2853    95    24]\n",
      " [   23   161    51   286   398     3     2    34    10     5]\n",
      " [   72   102    96  2528   425     3     2  2963     6 11497]\n",
      " [  427   956     3     2   510    10    86   912    18     3]\n",
      " [    2    34  4803    11 14125     4   272     3     2   251]\n",
      " [    4    61    10     3     2   329    22   519  2392    10]\n",
      " [  330     3     2  9630    24   136   547    16     3     2]\n",
      " [   50   129 12188   266    40    15    18     3     2    20]\n",
      " [  104    62   782     5    38    43   367     3     2   173]\n",
      " [    3     2  8140     7  1020     4   351    23     3     2]\n",
      " [  270  1358     3     2   411   503     8   110   513     3]\n",
      " [    2     7  7155    38    50    35  3040     3     2   739]\n",
      " [   50  4849   267    34    90     3     2   179   174   768]\n",
      " [   35   405    16    10     3     2    25   339    35   117]\n",
      " [  327  9386   970     3     2     7  9386    66   138  7834]\n",
      " [  163   511     3     2  1499     4   806     4   220     5]\n",
      " [  332    30     8   232    19   490   216     3     2    55]\n",
      " [  121  1312     3     2     8    22  3098   225   754     3]\n",
      " [    8   371     3     2     6   108     7  4131   275   867]\n",
      " [    2     7    46   104  3798   956     3     2     6   220]\n",
      " [   29     3     2   481   873   784     4   151   550   425]\n",
      " [    3     2    55    31   345    36     8   153   511     3]\n",
      " [  361     4  1391     3     2     7    38   716    60   174]\n",
      " [    2     6   409  6304   192  2945     3     2   104  1366]\n",
      " [    3     2     7    38   188   301  3833     3     2   140]\n",
      " [  816     4   186     3     2    11    89  1708   220    30]\n",
      " [   19     9    70   201 13014     3     2    11   384    31]\n",
      " [ 1185    14     3     2     5    62    20   579     9   240]\n",
      " [    3     2    66  3565   195     4   399  3912     3     2]\n",
      " [   41   722     3     2   120   373   220    21   126   722]\n",
      " [    8   474     3     2  1093     4  1925     4  1498     3]\n",
      " [  656 14065     3     2  5709    29   925   839     4  2716]], shape=(64, 10), dtype=int64)\n",
      "Target: tf.Tensor(\n",
      "[[  420 19286   129  1254    29     3     2    11    12  8847]\n",
      " [   32   756    22  1135     3     2  1246     4  2373    85]\n",
      " [  450  1158     3     2    76     8  3921   451   895   605]\n",
      " [ 5887  2493     8  8536     3     2    20 16443     5    88]\n",
      " [  598   458     3     2    14    22   532  2392 18665     4]\n",
      " [   91     3     2     6   239     4   973  1851  1316     3]\n",
      " [    2    11   144   587  1744    10  3301     3     2     5]\n",
      " [    2   863    29   760     4  2289     4  2087     3     2]\n",
      " [  155    51     3     2 15249   422    26  1894     3     2]\n",
      " [ 1925     4  5884     3     2   218     4   244     4  7596]\n",
      " [   90   212   143    90     3     2     9    22   168   244]\n",
      " [  895  8333   267     3     2    17 12076    12  2412     4]\n",
      " [    2   371     8   432  6279    47    29     3     2    11]\n",
      " [   17    13    12    38    70    79    74     3     2    34]\n",
      " [    3     2     5   286    94    90   493     3     2   165]\n",
      " [    3     2    36     8  1129    31  5296   145     3     2]\n",
      " [    4    43   773    14  1025     3     2   518    38    69]\n",
      " [   10     5  1556   575     3     2    11     9  3040  1370]\n",
      " [   34   235     8   161   168   511     3     2    15   500]\n",
      " [    5    86   356   117     4   104     3     2   359   685]\n",
      " [  232   504     3     2     7   689   502     4  5016   654]\n",
      " [   23  2170  4019   350     3     2  8678     8   595    24]\n",
      " [   20   156    18   985   362     3     2   291     4   461]\n",
      " [   38   188     8   370     4   403     3     2     7  1215]\n",
      " [    3     2     7  7000  3118   583   371     3     2  2211]\n",
      " [  394   826     3     2     5   584  1303     7   946   302]\n",
      " [ 5284   739     3     2     7    38  2375     8   790     4]\n",
      " [    3     2    98   105   235  1851     5   200     3     2]\n",
      " [   47   106  5340    18     3     2  5729    37    13   131]\n",
      " [    3     2    44    22    92     4   512  7900     3     2]\n",
      " [    2     5   155  1045  3374     3     2  3619  4398   356]\n",
      " [  337     3     2     6    23   533    29   952    17     3]\n",
      " [  558    43  1361     3     2     7  2853    95    24    30]\n",
      " [  161    51   286   398     3     2    34    10     5   624]\n",
      " [  102    96  2528   425     3     2  2963     6 11497 16224]\n",
      " [  956     3     2   510    10    86   912    18     3     2]\n",
      " [   34  4803    11 14125     4   272     3     2   251    35]\n",
      " [   61    10     3     2   329    22   519  2392    10     3]\n",
      " [    3     2  9630    24   136   547    16     3     2  7454]\n",
      " [  129 12188   266    40    15    18     3     2    20 11986]\n",
      " [   62   782     5    38    43   367     3     2   173  1331]\n",
      " [    2  8140     7  1020     4   351    23     3     2   333]\n",
      " [ 1358     3     2   411   503     8   110   513     3     2]\n",
      " [    7  7155    38    50    35  3040     3     2   739  4086]\n",
      " [ 4849   267    34    90     3     2   179   174   768   151]\n",
      " [  405    16    10     3     2    25   339    35   117   866]\n",
      " [ 9386   970     3     2     7  9386    66   138  7834  1477]\n",
      " [  511     3     2  1499     4   806     4   220     5   351]\n",
      " [   30     8   232    19   490   216     3     2    55   405]\n",
      " [ 1312     3     2     8    22  3098   225   754     3     2]\n",
      " [  371     3     2     6   108     7  4131   275   867     3]\n",
      " [    7    46   104  3798   956     3     2     6   220     5]\n",
      " [    3     2   481   873   784     4   151   550   425     3]\n",
      " [    2    55    31   345    36     8   153   511     3     2]\n",
      " [    4  1391     3     2     7    38   716    60   174   429]\n",
      " [    6   409  6304   192  2945     3     2   104  1366     8]\n",
      " [    2     7    38   188   301  3833     3     2   140    89]\n",
      " [    4   186     3     2    11    89  1708   220    30   332]\n",
      " [    9    70   201 13014     3     2    11   384    31   303]\n",
      " [   14     3     2     5    62    20   579     9   240    14]\n",
      " [    2    66  3565   195     4   399  3912     3     2    11]\n",
      " [  722     3     2   120   373   220    21   126   722     3]\n",
      " [  474     3     2  1093     4  1925     4  1498     3     2]\n",
      " [14065     3     2  5709    29   925   839     4  2716     3]], shape=(64, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_ids, target_ids in dataset.take(1):\n",
    "    print(\"Input:\", input_ids)\n",
    "    print(\"Target:\", target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4271ca6-85fa-476d-bd9e-1ae8206b8320",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The **MyModel** class is a custom TensorFlow/Keras model for text generation. It comprises three main layers:\n",
    "\n",
    "- **Embedding Layer:** Maps input token indices to dense embedding vectors.\n",
    "- **GRU Layer:** Processes the embedded sequences, producing output sequences and new states.\n",
    "- **Dense Layer:** Converts GRU output sequences to logits over the vocabulary.\n",
    "\n",
    "The `call` method defines the forward pass of the model, taking input sequences and optionally initial states, and returning output logits and new states if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fd8ecc-8d59-42b4-9f0a-7310a9c679c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.models.Model):\n",
    "    def __init__(self, vocab_size, embd_dim, rnn_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embd_dim)\n",
    "        self.gru = GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        if states == None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "            \n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x)\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141988dc-4cba-4ccc-98d9-4789c267cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = text_vec.vocabulary_size()\n",
    "EMBD_DIM = 512\n",
    "RNN_UNITS = 1048\n",
    "model = MyModel(VOCAB_SIZE, EMBD_DIM, RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44c3a4aa-24d9-4373-a4ad-dc930d295669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10, 19500)\n"
     ]
    }
   ],
   "source": [
    "for input_ids, target_ids in dataset.take(1):\n",
    "    pred = model(input_ids)\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ea89e-411b-49b3-974a-49a41ced8b50",
   "metadata": {},
   "source": [
    "In the above example the sequence length of the input is 100 but the model can be run on inputs of any length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91dd20d9-9e49-46a2-b837-166502cbbe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  9984000   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  4910928   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  20455500  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35350428 (134.85 MB)\n",
      "Trainable params: 35350428 (134.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21db691-0e6b-4ba7-bc34-cd1af0140d31",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee0e57bd-de30-405b-8b04-aa1a6cdbbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20706df-9a84-4b11-b862-1e61280e9ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717410900.754582    6651 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081/1081 [==============================] - 31s 28ms/step - loss: 5.0449\n",
      "Epoch 2/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 4.0436\n",
      "Epoch 3/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 3.5167\n",
      "Epoch 4/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 3.0462\n",
      "Epoch 5/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 2.6190\n",
      "Epoch 6/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 2.2516\n",
      "Epoch 7/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 1.9414\n",
      "Epoch 8/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 1.6866\n",
      "Epoch 9/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 1.4837\n",
      "Epoch 10/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 1.3207\n",
      "Epoch 11/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 1.1992\n",
      "Epoch 12/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 1.1068\n",
      "Epoch 13/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 1.0375\n",
      "Epoch 14/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.9886\n",
      "Epoch 15/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.9532\n",
      "Epoch 16/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.9228\n",
      "Epoch 17/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 0.8996\n",
      "Epoch 18/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8824\n",
      "Epoch 19/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8671\n",
      "Epoch 20/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8525\n",
      "Epoch 21/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 0.8415\n",
      "Epoch 22/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8350\n",
      "Epoch 23/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 0.8258\n",
      "Epoch 24/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8199\n",
      "Epoch 25/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8127\n",
      "Epoch 26/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8085\n",
      "Epoch 27/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 0.8071\n",
      "Epoch 28/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.8028\n",
      "Epoch 29/30\n",
      "1081/1081 [==============================] - 29s 26ms/step - loss: 0.7978\n",
      "Epoch 30/30\n",
      "1081/1081 [==============================] - 29s 27ms/step - loss: 0.7931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x75334ef70990>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf257ff-4f78-42c6-9d02-8adc2b13e74d",
   "metadata": {},
   "source": [
    "## Text Genaration\n",
    "\n",
    "Generating text with a word-level model involves iterating through a loop to predict the next word based on the preceding words. \n",
    "\n",
    "It starts with an `initial text`, an initial phrase or sentence. This initial text is fed into the model, which then predicts the subsequent word. The predicted word is appended to the initial text, and the process repeats. Each iteration adds one word to the generated text, gradually forming a coherent sequence. This iterative approach continues until the desired length of text is generated or until a stopping condition is met. By predicting words based on context, the model learns to produce text that mirrors the style and content of the training data, resulting in coherent and meaningful passages.\n",
    "\n",
    "\n",
    "![Alt text](download.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0126f083-8306-4578-b8f0-8f5086b74b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step(keras.models.Model):\n",
    "    def __init__(self, model, text_vec, temperature=1.0):\n",
    "        super(One_Step, self).__init__()\n",
    "        self.model = model\n",
    "        self.text_vec = text_vec\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        skip_ids = tf.constant([text_vec.get_vocabulary().index('[UNK]')], dtype=tf.int64)\n",
    "        skip_ids = tf.reshape(skip_ids, [-1, 1])\n",
    "        \n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        dense_shape=[text_vec.vocabulary_size()])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function()\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "\n",
    "        inputs = tf.constant([inputs])\n",
    "        ids = self.text_vec(inputs)\n",
    "        predicted, states = model(inputs=ids, states=states, return_state=True)\n",
    "        predicted = predicted[:, -1, :]\n",
    "        predicted = predicted / self.temperature\n",
    "        predicted = predicted + self.prediction_mask\n",
    "\n",
    "        predicted_id = tf.random.categorical(predicted, num_samples=1)\n",
    "        predicted_id = tf.squeeze(predicted_id, axis=-1)\n",
    "        \n",
    "        return predicted_id, states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3080620-5e58-49be-8b97-55753911c05d",
   "metadata": {},
   "source": [
    "## Text Generation Using One Step Model\n",
    "\n",
    "This code generates text using the one-step text generation model (one_step_model).\n",
    "\n",
    "It initializes with a starting phrase, `\"به نام خداوند\"` (In the name of God). Then, it iteratively calls the generate_one_step method of the model to predict the next word based on the previous words. The generated words are appended to the result list. After generating the desired number of words (300 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49138460-f466-478b-b1e3-066deb22b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = One_Step(model, text_vec, temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2b57a86-898f-487d-aa37-fb9429cfa5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " به نام خداوند  فر و مهان \n",
      " | به بیدادگر بر جهان شهریار \n",
      " | که آمد که پیروز باد مرا \n",
      " | به خورشید پیمان که آمد به ماه \n",
      " | که آباد تاج پدر یادگار \n",
      " | ز جمشید بر هر سویی شاد دار \n",
      " | چو کم شد به ایران ز فرزند شاه \n",
      " | که این بد که بر کوی تن شهریار \n",
      " | به گیتی چنین بود زرد و به روز \n",
      " | مرا دل بر و گیتی پراگنده گشت \n",
      " | بگفت این و دو دیده بر دست رفت \n",
      " | چنین رفت بر دیدهبان بر بدرید خویش \n",
      " | چنین گفت بر سان سخن راز بود \n",
      " | به گیتی ز ترکان کسی بینیاز \n",
      " | به من کردگار جهان بینیاز \n",
      " | به شبگیر کی بود و آمد به جای \n",
      " | گمان ای شما بر خرد \n",
      " | سخن با بزرگ و به جنگ \n",
      " | سخن گر برو بر لب جویبار \n",
      " | کمان را چو گرد کردم به دشت \n",
      " | بزد کوس و شکسته سواری آورید \n",
      " | زمین را کنون او سر اندر هوا \n",
      " | چو بشنید شاه این سخن راست شد \n",
      " | بر جهان آفرین برین باد و بر \n",
      " | جهان یکسر اندر جهان آورید \n",
      " | بر این بر یکی بیژن و بردش نماز \n",
      " | یکی تیر بر داده جان او تیر \n",
      " | یکی ابر و بر زد یکی پیشکار \n",
      " | ابا جای بر جای بر سر گرفت \n",
      " | بران نامور اندر اندر جهان \n",
      " | بر اندر هوا کوه چون بر زمین \n",
      " | هوا پر ز فرمان و دل پر ز جوش \n",
      " | هوا بر زمین و تیره بر \n",
      " | بر آواز بنشست کای نامور \n",
      " | همانگه پس اندر جهان آفرین \n",
      " | بگو جای خود \n",
      "\n",
      "\n",
      "\n",
      "Run time: 12.598061084747314\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = ' به نام خداوند '\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(300):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    next_char = ids_to_text(next_char)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result, separator=' ')\n",
    "end = time.time()\n",
    "print(result.numpy().decode('utf-8'), '\\n\\n')\n",
    "print('\\nRun time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ab86f-3c99-4870-992d-3334785eb2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
